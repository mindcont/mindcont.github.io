<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Kinect骨骼提取原理 | 项脊轩</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/3.0.3/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kinect骨骼提取原理</h1><a id="logo" href="/.">项脊轩</a><p class="description">项脊轩，旧南阁子也</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kinect骨骼提取原理</h1><div class="post-meta">May 10, 2016<span> | </span><span class="category"><a href="/categories/Kinect/">Kinect</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2016/05/10/real-time-human-pose-recognition-in-parts-from-single-depth-images/" href="/2016/05/10/real-time-human-pose-recognition-in-parts-from-single-depth-images/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#深度成像"><span class="toc-number">2.1.</span> <span class="toc-text">深度成像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#运动捕捉数据"><span class="toc-number">2.2.</span> <span class="toc-text">运动捕捉数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#生成合成数据"><span class="toc-number">2.3.</span> <span class="toc-text">生成合成数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">人体关节推理和联合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#人体关节标记"><span class="toc-number">3.1.</span> <span class="toc-text">人体关节标记</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度图像特征"><span class="toc-number">3.2.</span> <span class="toc-text">深度图像特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机决策森林"><span class="toc-number">3.3.</span> <span class="toc-text">随机决策森林</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#联接关节"><span class="toc-number">3.4.</span> <span class="toc-text">联接关节</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定性结果"><span class="toc-number">4.1.</span> <span class="toc-text">定性结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类准确度"><span class="toc-number">4.2.</span> <span class="toc-text">分类准确度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#训练图像数量"><span class="toc-number">4.2.1.</span> <span class="toc-text">训练图像数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#轮廓图像"><span class="toc-number">4.2.2.</span> <span class="toc-text">轮廓图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#树的深度"><span class="toc-number">4.2.3.</span> <span class="toc-text">树的深度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最大探针偏移"><span class="toc-number">4.2.4.</span> <span class="toc-text">最大探针偏移</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#联合预测精度"><span class="toc-number">4.3.</span> <span class="toc-text">联合预测精度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="post-content"><p>我们给出了从单深度图像中快速准确预测出人体关节3D位置的新方法，且没有使用时间信息。Kinect获取人体骨骼信息的大致原理可以概括为：<a id="more"></a><br>
1、使用物体识别方法，<strong>设计了身体组件的中间层表示</strong>，从而将困难的姿势估计问题映射为简单些的逐像素分类问题。2、大型、丰富多样的训练数据集保证了<strong>分类器(随机决策森林)估计</strong>身体组件时具有姿势、身材、衣着等不变性。3、通过<strong>重投影分类器</strong>的(身体组件估计结果)，我们生成了几个人体关节的可信3D估计。</p>
<h1>简介</h1>
<p>一些系统通过帧到帧的跟踪获得了很高的速度，但苦于<strong>无法快速重新初始化因而不鲁棒</strong>。本文中，我们专注于组件的姿势识别：<strong>从单深度图像中检测出每个骨骼关节的少量3D候选位置</strong>。我们设计了每帧初始化和恢复的技术，以作为任何适当跟踪算法的补充，从而可以进一步融入时间和运动的一致性中。这里给出的算法形成了Kinect游戏平台的核心组件。<br>
<img src="/images/Kinect-Skeleton-1.png" alt=""><br>
如图1所示，受近来物体识别工作将物体分成多个组件策略启发，我们的方法受两个关键设计目标驱动：<br>
<strong>1、计算高效且鲁棒</strong>。单幅输入深度图像被分割成稠密概率身体组件标签，组件定义为与感兴趣骨骼关节空间上相近的身体部分。<br>
<strong>2、将推理出的组件重投影到世界空间</strong>，我们局部化每个组件分布的空间模式，从而形成每个骨骼关节3D位置的带可信权重的预测，可能有几个。</p>
<p><strong>我们将身体组件的分割(从身体分割出各组件)当作逐像素分类问题</strong>。对每个像素分别评估避免了不同身体关节间的组合搜索，尽管单个身体组件在不同情形下的外观仍千差万别。<strong>我们从运动捕捉数据库中采样出不同身材和体型人物的各种姿势（人体的深度图），然后生成逼真的合成深度图作为训练数据。我们训练出了一个深随机决策森林分类器，为避免过拟合，我们使用了数十万幅训练图像</strong>。区别式深度比较图像特征简单产生3D变换不变性的同时维持了计算的高效性。为获得更高的速度，可以使用GPU在每个像素上并行运行分类器。推理出的逐像素分布的空间模式使用mean shift计算，由此空间模式给出3D关节的预测。</p>
<p>我们的主要贡献是：<strong>使用新颖的身体组件中间表示将姿势估计问题变成了物体识别问题，为低计算代价和高精度从空间上定位感兴趣的关节而设计了这个中间表示</strong>。<br>
我们从实验也获得了几个启示：<br>
(i)合成深度训练数据是真实数据的极好代理（代替品）；<br>
(ii)用各种合成数据成比例增大学习问题对（获得）高精确性很重要；<br>
(iii)我们基于组件的方法甚至比精妙的确切最近邻方法更通用。</p>
<h1>数据</h1>
<p>姿势估计研究往往关注克服训练数据缺乏的技术，这是因为两个问题。<strong>第一，使用计算机图形学技术生成逼真的强度图像往往受限于衣服、头发和皮肤造成的颜色和纹理的极大多变性，从而往往使生成的图像退化为2D轮廓</strong>。尽管深度摄像机极大地减小了这种困难，仍然存在相当可观的身体和服装shape变化。<strong>第二个限制是合成身体姿势图像需要以动作捕获（mocap）的数据作为输入</strong>。尽管存在模拟人类运动的技术，却无法模拟人类的所有自主运动。</p>
<h2 id="深度成像">深度成像</h2>
<p>深度摄像机较传统强度传感器有几个<strong>优势</strong>：工作光强水平低，提供校准后的尺度估计，具有颜色和纹理不变性，解决了姿势的轮廓模糊问题。<strong>它们还极大简化了背景减除操作，本文我们将这一点作为前提之一</strong>。对我们的方法更重 要的是，我们可以直接合成人物的逼真深度图像，从而可以轻易的建起大型的训练数据集。</p>
<h2 id="运动捕捉数据">运动捕捉数据</h2>
<p>人体可以做出很多姿势，这些是很难模仿的。<strong>因此，我们捕获人类运动构成一个大型运动捕获（mocap）数据库</strong>。我们的目的是包含人们在娱乐场景下所能做的所有姿势。数据库包含几百段内容为驾驶、跳舞、踢、跑、和导航菜单等的视频序列的约500k帧图像。</p>
<p><strong>我们希望我们半局部的身体组件分类器多少能推广到未见过的姿势</strong>。特别的，我们并不需要记录不同肢体的所有可能组合；实际上已证实较多的各种姿势已经足够了。进一步的，我们不需要记录运动捕获（mocap）关于垂直轴的旋转变化、左右镜像、场景位置、身材和体型、或摄像机位置，所有这些都可以（半）自动添加。</p>
<p>因为分类器没有使用时间信息，<strong>我们关注静止的姿势而不是运动</strong>。通常，一macap帧到下一帧之间的姿势变化小得可以忽略。因此，我们使用“最远邻(furthest neighbor)”聚集[15]从初始mocap数据中除去大量相似、冗余的姿势，“最远邻”聚集将姿势p1和p2之间的距离定义为，<img src="/images/Kinect-formula-1.png" alt="">即身体关节j的最大欧氏距离。我们使用100k姿势的子集以确保任何两个姿势之间的距离不小于5cm。</p>
<p>为了使用尚未发现的姿势空间区域提炼mocap（运动捕获）数据库，<strong>我们发现必须迭代执行包括运动捕获、从我们的模型采样、训练分类器和测试关节预测准确性的过程</strong>。我们早期的实验使用了CMU运动捕获数据库。尽管覆盖的姿势空间远远不够，它还是给出了可接受的结果。</p>
<h2 id="生成合成数据">生成合成数据</h2>
<p><strong>我们建立了一个随机渲染管道，从中我们可以对全标注训练图像集采样</strong>。我们建立该管道有两个目的：<strong>真实性和多样性</strong>。为使训练出的模型良好工作，采样必须与真实摄像机图像十分相似，并且良好覆盖我们在测试时希望识别的外观多样性。我们的特征对深度/尺度和平移变化都进行了显式处理（见下述），但是其它不变性没能有效编码。因此，我们从**（训练）数据学习摄像机、姿势、体型和身材的不变性**。</p>
<p><strong>合成管道首先随机采样一组参数，然后使用标准计算机图形学技术从纹理映射3D网络渲染深度和（见下述）身体组件图像</strong>。使用文献[4]中的方法，运动捕获重新指向覆盖身材和体型的15个基础网格。在身高和体重上使用的进一步轻微随机变化覆盖了额外的身材可变性。其它随机参数包括mocap帧、摄像机姿势、摄像机噪声、服装和发型。在补充材料中我们给出了这些变化的更多细节。图2比较了管道的各种输出与手工标注的摄像机图像。<br>
<img src="/images/Kinect-Skeleton-2.png" alt=""></p>
<h1>人体关节推理和联合</h1>
<p>在本节给出我们的身体组件中间表示、描述区别式深度图像特征、回顾决策森林及其在身体组件识别中的应用，最后讨论怎样使用一个模式(mode)发现算法生成关节位置的估计（proposals）。</p>
<h2 id="人体关节标记">人体关节标记</h2>
<p>本文的一个主要贡献是我们的<strong>身体组件中间表示</strong>。我们定义了稠密覆盖身体的几个局部身体组件标签，如图2的颜色编码。<strong>一些组件定义是用来直接定位感兴趣的特定骨架关节的，其他的是用来填补身体空白或者通过组合来预测其他关节的</strong>。我们的中间表示将问题转化成一个能很容易使用<strong>高效分类算法</strong>解决的问题。在4.3节我们证明了这种转换的惩罚代价很小。</p>
<p>组件在纹理映射中描述，纹理映射融合了渲染时的各种特征。<strong>深度和身体组件图像对作为全标注数据来训练分类器</strong>(见下述)。本文中的实验使用了31个人体组件：LU/RU/LW/RW头，颈，L/R肩，LU/RU/LW/RW手臂,L/R肘,L/R腕,L/R手,LU/RU/LW/RW躯干,LU/RU/LW/RW腿,L/R膝,L/R踝,L/R脚(Left, Right, Upper, loWer)。明确左右组件使分类器可以区分身体的左右侧。</p>
<p>当然，为适用特定的应用，这些组件的精确定义可以修改。如在上半身跟踪场景中，所有下半身的组件都可以去掉。组件应该充分小以准确定位身体关节，但是也不能太多以免浪费分类器的能力。</p>
<h2 id="深度图像特征">深度图像特征</h2>
<p>受文献[20]中使用特征启发，我们使用简单的深度比较特征。对于给定的像素x，特征计算如下：<br>
<img src="/images/Kinect-formula-2.png" alt=""><br>
其中dI(x)是图像I在像素x处的深度，参数Ø= (u; v)描述了偏移u和v。使用规范化偏移保证了特征是深度不变的：对身体的一个给定点，无论它离摄像机近还是远，（特征计算）都会给出一个固定的世界空间偏移。特征因此也是3D变换不变的（modulo perspective effects，模型的视角影响）。对背景中或图像边界之外的偏移像素，深度探针将给出一个大的正常数。<br>
<img src="/images/Kinect-Skeleton-3.png" alt=""><br>
图3图示了不同像素位置x的两个特征。特征（Ø1）对接近身体顶部位置的像素x会有较大正响应，但对较低身体位置处像素点的响应则接近0。而特征（Ø2）将有助于发现细竖直结构，如手臂。</p>
<p><strong>任意单个这样的特征都只能提供关于像素属于身体哪个组件的微弱信号，但是在决策森林中组合起来后，他们就足以准确区分所有训练组件</strong>。设计这些特征充分考虑到了计算效率：<strong>不需要预处理；（计算）每个特征最多只需要读取3个图像像素和执行5个算术运算；特征（计算）可以直接使用GPU实现</strong>。如果采用更大的计算代价，则可以采用潜在的性能更强的基于诸如区域深度积分、曲率的特征，或者使用局部描述符，如[5]中所采用的。<br>
<img src="/images/Kinect-Skeleton-4.png" alt=""></p>
<h2 id="随机决策森林">随机决策森林</h2>
<p>随机决策树和森林被证实是对于很多任务的快速有效多类分类器，且可以在GPU上高效实现；如图4所示，森林是T棵决策树的总体，每棵树都有分支节点和叶子节；每个分支都包含特征f(Ø)和一个阈值T组成；图像I的像素x进行分类时，从根节点开始不断计算公式1得到特征值，然后根据(特征值)与阈值T的比较结果往左或者往右分支。树T的叶子节点存储了身体组件标签c的训练出的分布。对所有树的分布求平均值并作为最终的分类：<br>
<img src="/images/Kinect-formula-3.png" alt=""></p>
<p><strong>训练</strong></p>
<p>每棵树都在一个<strong>不同的随机的合成样本库上训练</strong>得到。<strong>从每幅图像中随机选择2000个样本像素构成大致在各身体组件上均匀分布的随机子集</strong>。<br>
每棵树都使用下面的算法训练：</p>
<ul>
<li>1.随机给出一组（a set of）分支候选（特征参数和阈值);</li>
<li>2.使用每个将样本集分成左子集和右子集;<br>
<img src="/images/Kinect-formula-4.png" alt=""></li>
<li>3.通过求解最大信息增益问题确定：<br>
<img src="/images/Kinect-formula-5.png" alt="">其中香农熵(Shannon entropy)在所有的身体组件标签的规范化直方图上进行计算</li>
<li>4.如果最大增益仍然很大（足够大），并且树的深度没有达到最大值，则在左右子集和中继续递归。<br>
为降低训练时间，我们采用了分布式实现。在1000个核的集群上从1百万幅图像中将3棵树训练到20层花了大约1天。</li>
</ul>
<h2 id="联接关节">联接关节</h2>
<p><strong>前述身体组件识别推理出逐像素信息。现在需要将所有像素的这些信息汇聚起来形成3D骨架关节位置的可靠预测</strong>。这些预测是我们算法的最终输出，可以在跟踪算法中进行自初始化和从失败中恢复。</p>
<p><strong>一个简单的选择是使用知名的校准深度为每个组件累加概率(分布)团的全局3D中心</strong>。然而，无关像素会严重降低这样一个全局估计的质量。因此，我们采用了基于带权高斯核均值转移（mean shift）[10]的局部模式发现方法。</p>
<p>我们定义了如下的（逐个）身体组件的密度估计量：<br>
<img src="/images/Kinect-formula-6.png" alt=""><br>
其中是X 是3D世界空间中的坐标，N是图像像素数量，Wic是像素权重，Xi是图像像素xi在给定深度d（Xi）到世界空间的重新投影。Bc是训练出的每个组件的宽度。像素权重同时考虑了在像素上推理出的（属于哪个身体组件的）概率和像素在世界空间中的表面积：<br>
<img src="/images/Kinect-formula-7.png" alt=""><br>
这保证了密度估计是深度不变的，而且使关节预测的准确性有了一个虽小却意义重大的提高。根据身体组件定义不同，可以通过在少数组件集合中预累加得到后验概率P(c|I,x)。例如，在我们的实验中，可以融合覆盖头的四个身体组件来定位头关节。</p>
<p><strong>使用mean shift在这个概率密度（估计）中高效发现模式</strong>。对于组件c，我们训练出一个概率阈值λc，所有概率高于该阈值的像素都作为（mean shift）的起始点。当像素权重之和达到每个模式（mode）时就得到了最终的可信估计。这被证明比采用模态密度估计更可靠。</p>
<p><strong>检测出的模式（实际上）位于身体的表面</strong>。因此，使用训练出的z偏移将每个模式还原到现场（即身体表面）从而产生最后的关节位置预测。这个简单高效的方法在实践中工作得很好。宽度Bc，概率阈值λc，和表面到内部的z偏移都在一个保留的5000幅图像的验证集中使用网格搜索（grid search）优化。（As an indication,得到的平均宽度为0.065m，概率阈值为0.14，z偏移为0.039m。）</p>
<h1>实验</h1>
<p>本节描述评估我们方法性能所进行的实验。我们给出了在几个具有挑战性的数据集上的定性和定量结果，并与最近邻方法和当前最高水准[13]进行了比较。在补充材料中我们给出了进一步的结果。除非特别指明，以下的参数都是如下都是这样设置的：3棵树，20层深，每棵树使用300k幅训练图像，每幅图像2000个训练样本像素，2000个候选特征θ，每个特征50个候选阈值τ。</p>
<h2 id="定性结果">定性结果</h2>
<p><img src="/images/Kinect-Skeleton-5.png" alt=""><br>
图5给出了我们算法的一些例子推理。<strong>请注意，在很大的身体和摄像机姿势、景深、cropping（裁剪）、身材和体型（如瘦小的小孩和壮硕的成年人）变化范围上都有很高的分类和关节预测准确性</strong>。底部一列给出了身体组件分类失败的一些形式（modes）。第一个例子是没能区分出<strong>深度图像中的细微变化</strong>，像交叉的手臂。常常（就像第二和第三个失败的例子）最可能的<strong>身体组件是不正确的</strong>，但是在分布P(c|I,x)中还是有足够多的正确概率团可以生成准确的（关节）预测。第四个是没能很好<strong>推广到未见过的姿势的例子</strong>。但是（设置好的）置信度防止了（gates）很差预测的出现，以再现率（recall）的损失为代价维持了高的精确性。</p>
<p>注意我们没有使用任何时间或运动约束。尽管如此，补充材料中视频序列每帧图像的结果表明几乎每个关节都被准确预测，且抖动惊人的小。</p>
<h2 id="分类准确度">分类准确度</h2>
<p><img src="/images/Kinect-Skeleton-6.png" alt=""><br>
我们研究了几个训练参数对分类准确性的影响。在合成测试集和真实测试集上的趋势高度相关，并且真实测试集一致地显得比合成测试集更“容易”，也是是因为其中的姿势变化少些。<br>
<img src="/images/Kinect-Skeleton-7.png" alt=""></p>
<h3 id="训练图像数量">训练图像数量</h3>
<p>在图6(a)中，我们展示了准确性大约与随机生成图像数量之间呈对数关系，尽管训练图像增加到大约100k幅时准确性开始停止提高。下面就会展示，这种饱和很可能是3棵20层决策树构成的森林本身模型能力有限所致。</p>
<h3 id="轮廓图像">轮廓图像</h3>
<p>图6(a)也给出了我们的方法在合成轮廓图像上表现出来的品质。在合成轮廓图像中，式1所计算出来的特征可以是带尺度（以平均深度度量）的、也可以是不带尺度的（固定深度）。采用2D衡量标准设定10像素真正（true positive）阈值，对应的预测准确性分别是：有尺度的为0.539mAP，不带尺度的为0.465mAP。尽管深度模糊明显使预测更艰难，这些结果仍表明了我们方法对其他图像形态（other imaging modalities）也有较好的适用性。</p>
<h3 id="树的深度">树的深度</h3>
<p>使用15k和900k幅图像，图6(b)展示了树深度对测试准确性的影响。<strong>在所有训练参数中，深度似乎具有最显著的作用，因为它直接影响着分类器的建模能力</strong>。只使用15k幅训练图像时在17层左右就出现了过拟合，将训练集增多到900k则避免了过拟合。精度在20层处的较高梯度表明训练更多层能得到更好的结果，当然有附加的较小的计算代价加上较大的额外存储代价。从实用方面看，感兴趣的是，直到大约10层，训练集的大小关系不大，这告诉了我们一个高效训练策略。</p>
<h3 id="最大探针偏移">最大探针偏移</h3>
<p><strong>训练时允许的深度探针偏移范围对准确性有很大影响</strong>。图6©给出了在5k幅训练图像上的比较结果。这里，最大探针偏移(‘maximum probe offset’)指式1中u和v允许赋予的x和y坐标的最大绝对值。图中右侧的同心盒子是5个测试的最大偏移集，使用图像中左肩处的像素对它们进行了较准；最大的偏移几乎覆盖了整个身体。（记得最大偏移随像素的世界深度变化而缩放）。随着最大探针偏移增加，分类器可以使用更多空间上下文来做决定，尽管训练数据不够最终会导致过拟合，精度随最大探针偏移增大而提高，尽管在约129像素大小处就不再增大。</p>
<h2 id="联合预测精度">联合预测精度</h2>
<p><img src="/images/Kinect-Skeleton-8.png" alt=""><br>
图7给出了在合成测试集上的平均精度，达0.731mAP。我们对比了理想化的配置——即给出了真实身体组件标签的情况和使用推理出的身体组件的真实配置情况。我们确实付出了小小的代价来适用身体组件的中间表示，但是很多关节的推理结果有很高的准确性并且接近（理论上的）上界。在真实测试集上，我们有头、肩、肘和手的真实标签。在这些真实组件上mAP达0.984，而在推理出的身体组件上的mAP也达到了0.914。正如预期一样，在这个较易的测试集上，这些数据理所当然较高。</p>
<h1>参考</h1>
<p><strong>[1]《Real-Time Human Pose Recognition in Parts from Single Depth Images》</strong>-Jamie Shotton  Andrew Fitzgibbon,Microsoft Research Cambridge &amp; Xbox Incubation</p>
<p><i class="fa fa-download" aria-hidden="true"><a href="ftp://mindcont.com/Book/Real-Time%20Human%20Pose%20Recognition%20in%20Parts%20from%20Single%20Depth%20Images%20%E8%8B%B1%E6%96%87.pdf" target="_blank" rel="external">英文原文</a></i><br>
<i class="fa fa-download" aria-hidden="true"><a href="ftp://mindcont.com/Book/Real-Time%20Human%20Pose%20Recognition%20in%20Parts%20from%20Single%20Depth%20Images%20%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91.pdf " target="_blank" rel="external">中文翻译</a></i></p>
<p>来源：网络 编辑：<a href="https://github.com/mindcont" target="_blank" rel="external">mindcont</a> 校对：<a href="https://github.com/mindcont" target="_blank" rel="external">mindcont</a></p>
</div><div class="tags"><a href="/tags/Kinect/">Kinect</a></div><div class="post-nav"><a href="/2016/05/12/win10-ubuntu-remote-desktop/" class="pre">win10连结ubuntu 远程桌面</a><a href="/2016/05/09/the-basic-principle-of-kinect-depth-imaging/" class="next">Kinect深度成像的基本原理</a></div><div id="disqus_thread"><script>var disqus_shortname = 'micro-era';
var disqus_identifier = '2016/05/10/real-time-human-pose-recognition-in-parts-from-single-depth-images/';
var disqus_title = 'Kinect骨骼提取原理';
var disqus_url = 'http://blog.mindcont.com/2016/05/10/real-time-human-pose-recognition-in-parts-from-single-depth-images/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//micro-era.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://blog.mindcont.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kinect/">Kinect</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/深度学习/">深度学习</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔日记/">随笔日记</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/树莓派/" style="font-size: 15px;">树莓派</a> <a href="/tags/coding/" style="font-size: 15px;">coding</a> <a href="/tags/caffe/" style="font-size: 15px;">caffe</a> <a href="/tags/OpenCV/" style="font-size: 15px;">OpenCV</a> <a href="/tags/TED/" style="font-size: 15px;">TED</a> <a href="/tags/Kinect/" style="font-size: 15px;">Kinect</a> <a href="/tags/matlab/" style="font-size: 15px;">matlab</a> <a href="/tags/ros/" style="font-size: 15px;">ros</a> <a href="/tags/转载/" style="font-size: 15px;">转载</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/04/25/how-to-do-mnist-demo/">Caffe 入门-MNIST 实验</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/25/debug-android-win/">调试安卓的小技巧</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/22/some-advice-for-research-beginners/">给初做科研的一些建议</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/20/visual-cortex/">从视网膜到视皮层——视觉系统知多少</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/16/my-review-2016/">2016 , 我的年终总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/15/ideas-worth-spreading/">值得传播的思想</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/02/15/caffe-guide-book/">Caffe 入门指南</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/01/05/machine-learning-for-software-engineers/">自上而下的学习路线-软件工程师的机器学习</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/26/nginx-proxy-lan/">局域网内nginx反向代理上网</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/14/ros-naoqi-ubuntu/">naoqi ros 开发环境搭建</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//micro-era.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://memect.com/" title="好东西传送门" target="_blank">好东西传送门</a><ul></ul><a href="https://prism-break.org/zh-CN/all/" title="粉碎棱镜" target="_blank">粉碎棱镜</a><ul></ul><a href="https://www.daoon.com/" title="道卬" target="_blank">道卬</a><ul></ul><a href="https://www.ted.com/" title="TED | 值得传播的思想" target="_blank">TED | 值得传播的思想</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">项脊轩.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-74386502-3','auto');ga('send','pageview');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>