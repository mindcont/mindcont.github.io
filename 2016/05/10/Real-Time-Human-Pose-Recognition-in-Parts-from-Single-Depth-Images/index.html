<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Kinect骨骼提取原理 | 微纪元</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/3.0.3/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/2.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Kinect骨骼提取原理</h1><a id="logo" href="/.">微纪元</a><p class="description">根据同名科幻小说改编</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/history/"><i class="fa fa-history"> 历史</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Kinect骨骼提取原理</h1><div class="post-meta">May 10, 2016<span> | </span><span class="category"><a href="/categories/Kinect/">Kinect</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-disqus-identifier="2016/05/10/Real-Time-Human-Pose-Recognition-in-Parts-from-Single-Depth-Images/" href="/2016/05/10/Real-Time-Human-Pose-Recognition-in-Parts-from-Single-Depth-Images/#disqus_thread" class="disqus-comment-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#简介"><span class="toc-number">2.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#数据"><span class="toc-number">3.</span> <span class="toc-text">数据</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#深度成像"><span class="toc-number">3.1.</span> <span class="toc-text">深度成像</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#运动捕捉数据"><span class="toc-number">3.2.</span> <span class="toc-text">运动捕捉数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#生成合成数据"><span class="toc-number">3.3.</span> <span class="toc-text">生成合成数据</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#人体关节推理和联合"><span class="toc-number">4.</span> <span class="toc-text">人体关节推理和联合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#人体关节标记"><span class="toc-number">4.1.</span> <span class="toc-text">人体关节标记</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深度图像特征"><span class="toc-number">4.2.</span> <span class="toc-text">深度图像特征</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机决策森林"><span class="toc-number">4.3.</span> <span class="toc-text">随机决策森林</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#联接关节"><span class="toc-number">4.4.</span> <span class="toc-text">联接关节</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#实验"><span class="toc-number">5.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#定性结果"><span class="toc-number">5.1.</span> <span class="toc-text">定性结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类准确度"><span class="toc-number">5.2.</span> <span class="toc-text">分类准确度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#训练图像数量"><span class="toc-number">5.2.1.</span> <span class="toc-text">训练图像数量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#轮廓图像"><span class="toc-number">5.2.2.</span> <span class="toc-text">轮廓图像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#树的深度"><span class="toc-number">5.2.3.</span> <span class="toc-text">树的深度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#最大探针偏移"><span class="toc-number">5.2.4.</span> <span class="toc-text">最大探针偏移</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#联合预测精度"><span class="toc-number">5.3.</span> <span class="toc-text">联合预测精度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#与最近邻（方法）比较"><span class="toc-number">5.3.1.</span> <span class="toc-text">与最近邻（方法）比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#与-13-比较"><span class="toc-number">5.3.2.</span> <span class="toc-text">与[13]比较</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#全旋转和多人情况"><span class="toc-number">5.3.3.</span> <span class="toc-text">全旋转和多人情况</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#快速预测"><span class="toc-number">5.3.4.</span> <span class="toc-text">快速预测</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#探讨"><span class="toc-number">6.</span> <span class="toc-text">探讨</span></a></li></ol></div></div><div class="post-content"><a id="more"></a> 
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们给出了从单深度图像（a single depth image）中快速准确预测出人体关节3D位置的新方法，且没有使用时间信息。我们使用物体识别方法，设计了身体组件（part）的中间层表示，从而将困难的姿势估计问题映射为简单些的逐像素分类问题（per-pixel classification problem）。大型、丰富多样的训练数据集保证了分类器估计身体组件时具有姿势、身材、衣着等不变性。最后，通过重投影分类器的（身体组件估计）结果，我们生成了几个人体关节的可信3D估计（proposals）。系统在消费硬件上能以200帧每秒的速度运行。我们的评估在合成和真实测试集上都高度准确，我们同时也研究了几个训练参数的作用。与相关工作相比，我们取得了最好的准确性，并且比“确切全骨架最近邻”匹配(exact whole-skeleton nearest neighbor matching)具有更好的通用性。</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>鲁棒互动人体跟踪的应用包括游戏、人机交互、安全、远程呈现和健康保健等。近来由于实时深度摄像机的 引入，这个问题得到了极大的简化[16，19，44，37，28，12]。然而，即使现在的最好系统也有局限性。特别是，直到Kinect[21]（Kinect是微软在2010年6月14日对XBOX360体感周边外设正式发布的名字。）发布，还没有任何一个系统能在消费硬件上以互动速度（interactive rates）处理正在进行各种运动(general body motions)的所有身材和体型的人体。一些系统通过帧到帧的跟踪获得了很高的速度，但苦于无法快速重新初始化因而不鲁棒。本文中，我们专注于组件的姿势识别(pose recognition in parts)：从单深度图像中检测出每个骨骼关节的少量3D候选位置。我们设计了每帧初始化和恢复的技术，以作为任何适当跟踪算法[7，39，16，42，13]的补充，从而可以进一步融入时间和运动的一致性中。这里给出的算法形成了Kinect游戏平台[21]的核心组件。<br><img src="/images/Kinect-Skeleton-1.png" alt=""><br>如图1所示，受近来物体识别工作将物体分成多个组件策略（如[12，43]）启发，我们的方法受两个关键设计目标驱动：计算高效且鲁棒。单幅输入深度图像被分割成稠密概率身体组件标签，组件定义为与感兴趣骨骼关节空间上相近的身体部分。将推理出的组件重投影到世界空间，我们局部化每个组件分布的空间模式，从而形成每个骨骼关节3D位置的带可信权重的预测（proposals，可能有几个）。</p>
<p>我们将身体组件的分割(从身体分割出各组件)当作逐像素分类问题 (no pairwise terms or CRF have proved necessary)。对每个像素分别评估避免了不同身体关节间的组合搜索，尽管单个身体组件在不同情形下的外观仍千差万别。我们从运动捕捉数据库中采样出不同身材和体型人物的各种姿势（人体的深度图），然后生成逼真的合成深度图作为训练数据。我们训练出了一个深随机决策森林分类器，为避免过拟合，我们使用了数十万幅训练图像。区别式深度比较图像特征简单产生3D变换不变性的同时维持了计算的高效性。为获得更高的速度，可以使用GPU在每个像素上并行运行分类器[34]。推理出的逐像素分布的空间模式使用mean shift[10]计算，由此空间模式给出3D关节的预测。</p>
<p>我们算法的一个优化实现在Xbox360 GPU上能以每帧不超过5ms的速度运行（即200帧每秒），这比现有方法至少快一个数量级以上。该（算法的）实现一帧接一帧地运行，每帧中人物的身材和体型都很不相同。学习出的区别型方法自然地处理自遮挡和从帧图像抠出的姿势（and the learned discriminative approach naturally handles self-occlusions and poses cropped by the image frame.）。我们在真实和合成深度图像上（对算法）进行了评估，这些深度图像包含了各种人物的具有挑战性的姿势。甚至在没有使用时间或运动约束的情况下，3D关节的预测（proposals）也既精确又稳定。我们研究了几个训练参数的作用，指出了拥有大型训练集时，多深的（决策）树仍能避免过拟合。（实验）表明在理想环境和现实环境下，我们的（身体）组件预测至少与“确切最近邻”方法一样通用（We demonstrate that our part proposals generalize at least as well as exact nearest-neighbor in both an idealized and realistic setting），并且比现有技术水平有了实质提高。并且，在轮廓图像(silhouette images)上实验的结果表明我们的方法有更通用的应用。</p>
<p>我们的主要贡献是：使用新颖的身体组件中间表示将姿势估计问题变成了物体识别问题，为低计算代价和高精度从空间上定位感兴趣的关节而设计了这个中间表示。我们从实验也获得了几个启示：( I )合成深度训练数据是真实数据的极好代理（代替品）；(ii)用各种合成数据成比例增大学习问题对（获得）高精确性很重要；和(iii)我们基于组件的方法甚至比精妙的确切最近邻方法更通用。</p>
<p><strong>相关工作</strong><br>关于人类姿势的估计已经有了大量文献（[22，19]中有述评）。近来引入的深度摄像机进一步推动了研究的发展[16，19，28]。尤其是[28]与我们的方法最相似，Plagemann等人构建3D mesh（网络）来发现测地极值（geodesic extrema）兴趣点，这些兴趣点分类为3种组件：头、手和脚。他们的方法对各组件的位置和方向都进行了估计，但没区分左右，使用兴趣点也限制了组件的选择。</p>
<p>使用传统强度像机方面也取得了进展，尽管通常付出了更高的计算代价。Bregler和Malik[7]使用已知初始姿势的扭曲和指数地图（maps）跟踪人物。Ioffe和Forsyth[17]将平行边进行分组(group parallel edges as candidate body segments and prune)，并作为身体部分（segment）的候选，然后使用投影分类器裁剪这些身体部分（候选）的组合。Mori和Malik[24]使用形状上下文描述符匹配样本。Ramanan和Forsyth[31]将身体部分的 候选当作平行线对，然后在帧间聚集外观。Shakhnarovich等[33]估计上半身姿势，通过参数敏感哈希（散列）匹配插值k-NN姿势。Agarwal和Triggs[1]学习了一个从核化图像轮廓特征到姿势的回归（函数）。Sigal等[39]使用本征外观模板检测器估计头、上臂和小腿(low legs)。Felzenszwalb和Huttenlocher[11]运用图画结构高效地估计姿势。Navaratnam等使用未标注数据的边际统计提高姿势估计（的性能）。Urtasum和Darrel[41]提出使用高斯过程的局部混合来回归人物姿势。[40]使用自动上下文来获取粗糙的身体组件标签，但它不是用来定位关节的，并且给每帧分类时需要约40秒。Rogez等[32]在循环人类运动模式和摄像机角度集（torus）上定义了分类层次结构，然后基于该结构训练了随机决策森林。Wang和Popovi′c[42]跟踪了一只戴彩色手套的手。我们的系统可以视作从深度图象自动推理出虚拟彩色衣服的颜色。Bourdev和Malik[6]由3D姿势和2D图像外观的紧族（tight clusters）获取了“姿势群”，他们可以使用SVMs检测。</p>
<h1 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h1><p>姿势估计研究往往关注克服训练数据缺乏的技术[25]，这是因为两个问题。第一，使用计算机图形学技术[33，27，26]生成逼真的强度图像往往受限于衣服、头发和皮肤造成的颜色和纹理的极大多变性，从而往往使生成的图像退化为2D轮廓[1]。尽管深度摄像机极大地减小了这种困难，仍然存在相当可观的身体和服装shape变化。第二个限制是合成身体姿势图像需要以动作捕获（mocap）的数据作为输入。尽管存在模拟人类运动的技术（如[38]），却无法模拟人类的所有自主运动。</p>
<p>在本节我们回顾一下深度图像，并且解释了我们如何使用真实运动捕获数据生成各种基本角色模型，从而合成一个大型且多样化的数据集。我们相信这个数据集在规模和多样性方面都超过了现有水平，且实验表明这样大型的数据集在我们的评估中有多重要。</p>
<h2 id="深度成像"><a href="#深度成像" class="headerlink" title="深度成像"></a>深度成像</h2><p>深度图像技术在过去的几年中有了极大的发展，随着Kinect[21]的发布最终成为了大众消费品。深度图像中的像素记录（indicate）了场景的校准深度，而不是场景强度或颜色的值（measure）。我们使用的Kinect摄像机每秒能捕获640×480规格图像30帧，其深度分辨率为几厘米（a few centimeters）。</p>
<p>深度摄像机较传统强度传感器有几个优势：工作光强水平低，提供校准后的尺度估计（giving a calibrated scale estimate），具有颜色和纹理不变性，解决了姿势的轮廓模糊问题。它们还极大简化了背景减除操作，本文我们将这一点作为前提之一。对我们的方法更重 要的是，我们可以直接合成人物的逼真深度图像，从而可以轻易的建起大型的训练数据集。</p>
<h2 id="运动捕捉数据"><a href="#运动捕捉数据" class="headerlink" title="运动捕捉数据"></a>运动捕捉数据</h2><p>人体可以做出很多姿势，这些是很难模仿的。因此，我们捕获人类运动构成一个大型运动捕获（mocap）数据库。我们的目的是包含人们在娱乐场景下所能做的所有姿势。数据库包含几百段内容为驾驶、跳舞、踢、跑、和导航菜单等的视频序列的约500k帧图像。</p>
<p>我们希望我们半局部的身体组件分类器多少能推广到未见过的姿势。特别的，我们并不需要记录不同肢体的所有可能组合；实际上已证实较多的各种姿势已经足够了。进一步的，我们不需要记录运动捕获（mocap）关于垂直轴的旋转变化、左右镜像、场景位置、身材和体型、或摄像机位置，所有这些都可以（半）自动添加。</p>
<p>因为分类器没有使用时间信息，我们关注静止的姿势而不是运动。通常，一macap帧到下一帧之间的姿势变化小得可以忽略。因此，我们使用“最远邻(furthest neighbor)”聚集[15]从初始mocap数据中除去大量相似、冗余的姿势，“最远邻”聚集将姿势p1和p2之间的距离定义为，<img src="/images/Kinect-formula-1.png" alt="">即身体关节j的最大欧氏距离。我们使用100k姿势的子集以确保任何两个姿势之间的距离不小于5cm。</p>
<p>为了使用尚未发现的姿势空间区域提炼（refine）mocap（运动捕获）数据库，我们发现必须迭代执行包括运动捕获、从我们的模型采样、训练分类器和测试关节预测准确性的过程。我们早期的实验使用了CMU运动捕获数据库[9]。尽管覆盖的姿势空间远远不够，它还是给出了可接受的结果。</p>
<h2 id="生成合成数据"><a href="#生成合成数据" class="headerlink" title="生成合成数据"></a>生成合成数据</h2><p>我们建立了一个随机渲染管道，从中我们可以对全标注训练图像集采样。我们建立该管道有两个目的：真实性和多样性。为使训练出的模型良好工作，采样必须与真实摄像机图像十分相似，并且良好覆盖我们在测试时希望识别的外观多样性。我们的特征对深度/尺度和平移变化都进行了显式处理（见下述），但是其它不变性没能有效编码。因此，我们从（训练）数据学习摄像机、姿势、体型和身材的不变性。</p>
<p>合成管道首先随机采样一组参数，然后使用标准计算机图形学技术从纹理映射3D网络渲染深度和（见下述）身体组件图像。使用文献[4]中的方法，运动捕获重新指向覆盖身材和体型的15个基础网格。（The mocap is retargeting to each of 15 base meshes spanning the range of body shapes and sizes, using [4].）在身高和体重上使用的进一步轻微随机变化覆盖了额外的身材可变性。其它随机参数包括mocap帧、摄像机姿势、摄像机噪声、服装和发型。在补充材料中我们给出了这些变化的更多细节。图2比较了管道的各种输出与手工标注的摄像机图像。<br><img src="/images/Kinect-Skeleton-2.png" alt=""></p>
<h1 id="人体关节推理和联合"><a href="#人体关节推理和联合" class="headerlink" title="人体关节推理和联合"></a>人体关节推理和联合</h1><p>在本节给出我们的身体组件中间表示、描述区别式深度图像特征、回顾决策森林及其在身体组件识别中的应用，最后讨论怎样使用一个模式(mode)发现算法生成关节位置的估计（proposals）。</p>
<h2 id="人体关节标记"><a href="#人体关节标记" class="headerlink" title="人体关节标记"></a>人体关节标记</h2><p>本文的一个主要贡献是我们的身体组件中间表示。我们定义了稠密覆盖身体的几个局部身体组件标签，如图2的颜色编码。一些组件定义是用来直接定位感兴趣的特定骨架关节的，其他的是用来填补身体空白或者通过组合来预测其他关节的。我们的中间表示将问题转化成一个能很容易使用高效分类算法解决的问题。在4.3节我们证明了这种转换的惩罚代价很小。</p>
<p>组件在纹理映射中描述，纹理映射融合了渲染时的各种特征(The parts are specified in a texture map that is retargeted to skin the various characters during rendering)。深度和身体组件图像对作为全标注数据来训练分类器(见下述)。本文中的实验使用了31个人体组件：LU/RU/LW/RW头，颈，L/R肩，LU/RU/LW/RW手臂,L/R肘,L/R腕,L/R手,LU/RU/LW/RW躯干,LU/RU/LW/RW腿,L/R膝,L/R踝,L/R脚(Left, Right, Upper, loWer)。明确左右组件使分类器可以区分身体的左右侧。</p>
<p>当然，为适用特定的应用，这些组件的精确定义可以修改。如在上半身跟踪场景中，所有下半身的组件都可以去掉。组件应该充分小以准确定位身体关节，但是也不能太多以免浪费分类器的能力。</p>
<h2 id="深度图像特征"><a href="#深度图像特征" class="headerlink" title="深度图像特征"></a>深度图像特征</h2><p>受文献[20]中使用特征启发，我们使用简单的深度比较特征。对于给定的像素x，特征计算如下：<br><img src="/images/Kinect-formula-2.png" alt=""><br>其中dI(x)是图像I在像素x处的深度，参数Ø= (u; v)描述了偏移u和v。使用规范化偏移保证了特征是深度不变的：对身体的一个给定点，无论它离摄像机近还是远，（特征计算）都会给出一个固定的世界空间偏移。特征因此也是3D变换不变的（modulo perspective effects，模型的视角影响）。对背景中或图像边界之外的偏移像素，深度探针将给出一个大的正常数。<br><img src="/images/Kinect-Skeleton-3.png" alt=""><br>图3图示了不同像素位置x的两个特征。特征（Ø1）对接近身体顶部位置的像素x会有较大正响应，但对较低身体位置处像素点的响应则接近0。而特征（Ø2）将有助于发现细竖直结构，如手臂。</p>
<p>任意单个这样的特征都只能提供关于像素属于身体哪个组件的微弱信号，但是在决策森林中组合起来后，他们就足以准确区分所有训练组件。设计这些特征充分考虑到了计算效率：不需要预处理；（计算）每个特征最多只需要读取3个图像像素和执行5个算术运算；特征（计算）可以直接使用GPU实现。如果采用更大的计算代价，则可以采用潜在的性能更强的基于诸如区域深度积分、曲率的特征，或者使用局部描述符，如[5]中所采用的。<br><img src="/images/Kinect-Skeleton-4.png" alt=""></p>
<h2 id="随机决策森林"><a href="#随机决策森林" class="headerlink" title="随机决策森林"></a>随机决策森林</h2><p>随机决策树和森林[35，30，2，8]被证实是对于很多任务的快速有效多类分类器[20，23，36]，且可以在GPU[34]上高效实现；如图4所示，森林是T棵决策树的总体，每棵树都有分支节点和叶子节；每个分支都包含特征f(Ø)和一个阈值T组成；图像I的像素x进行分类时，从根节点开始不断计算公式1得到特征值，然后根据(特征值)与阈值T的比较结果往左或者往右分支。树T的叶子节点存储了身体组件标签c的训练出的分布。对所有树的分布求平均值并作为最终的分类：<br><img src="/images/Kinect-formula-3.png" alt=""><br><strong>训练</strong></p>
<p>每棵树都在一个不同的随机的合成样本库上训练得到。从每幅图像中随机选择2000个样本像素构成大致在各身体组件上均匀分布的随机子集。</p>
<p>每棵树都使用下面的算法训练[20]：</p>
<pre><code>* **1.随机给出一组（a set of）分支候选（特征参数和阈值);**
* **2.使用每个将样本集分成左子集和右子集;**
![](/images/Kinect-formula-4.png)
* **3.通过求解最大信息增益问题确定：**
![](/images/Kinect-formula-5.png)
其中香农熵(Shannon entropy)在所有的身体组件标签的规范化直方图上进行计算
* **4.如果最大增益仍然很大（足够大），并且树的深度没有达到最大值，则在左右子集和中继续递归。**
</code></pre><p>为降低训练时间，我们采用了分布式实现。在1000个核的集群上从1百万幅图像中将3棵树训练到20层花了大约1天。</p>
<h2 id="联接关节"><a href="#联接关节" class="headerlink" title="联接关节"></a>联接关节</h2><p>前述身体组件识别推理出逐像素信息。现在需要将所有像素的这些信息汇聚起来形成3D骨架关节位置的可靠预测。这些预测是我们算法的最终输出，可以在跟踪算法中进行自初始化和从失败中恢复。</p>
<p>一个简单的选择是使用知名的校准深度为每个组件累加概率(分布)团的全局3D中心。然而，无关像素会严重降低这样一个全局估计的质量。因此，我们采用了基于带权高斯核均值转移（mean shift）[10]的局部模式发现方法。</p>
<p>我们定义了如下的（逐个）身体组件的密度估计量：<br><img src="/images/Kinect-formula-6.png" alt=""><br>其中是X 是3D世界空间中的坐标，N是图像像素数量，Wic是像素权重，Xi是图像像素xi在给定深度d（Xi）到世界空间的重新投影。Bc是训练出的每个组件的宽度。像素权重同时考虑了在像素上推理出的（属于哪个身体组件的）概率和像素在世界空间中的表面积：<br><img src="/images/Kinect-formula-7.png" alt=""><br>这保证了密度估计是深度不变的，而且使关节预测的准确性有了一个虽小却意义重大的提高。根据身体组件定义不同，可以通过在少数组件集合中预累加得到后验概率P(c|I,x)。例如，在我们的实验中，可以融合覆盖头的四个身体组件来定位头关节。</p>
<p>使用mean shift在这个概率密度（估计）中高效发现模式。对于组件c，我们训练出一个概率阈值λc，所有概率高于该阈值的像素都作为（mean shift）的起始点。当像素权重之和达到每个模式（mode）时就得到了最终的可信估计。这被证明比采用模态密度估计更可靠。</p>
<p>检测出的模式（实际上）位于身体的表面。因此，使用训练出的z偏移将每个模式还原到现场（即身体表面）从而产生最后的关节位置预测。这个简单高效的方法在实践中工作得很好。宽度Bc，概率阈值λc，和表面到内部的z偏移都在一个保留的5000幅图像的验证集中使用网格搜索（grid search）优化。（As an indication,得到的平均宽度为0.065m，概率阈值为0.14，z偏移为0.039m。）</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>本节描述评估我们方法性能所进行的实验。我们给出了在几个具有挑战性的数据集上的定性和定量结果，并与最近邻方法和当前最高水准[13]进行了比较。在补充材料中我们给出了进一步的结果。除非特别指明，以下的参数都是如下都是这样设置的：3棵树，20层深，每棵树使用300k幅训练图像，每幅图像2000个训练样本像素，2000个候选特征θ，每个特征50个候选阈值τ。<br><strong>测试数据</strong><br>我们使用具有挑战性的合成和真实深度图像评估我们的方法。对于我们的合成测试集，我们合成了5000幅深度图像，并带有了真实的身体标签和关节位置。用来产生这些合成图象的原始mocap姿势（图像）从训练数据中剔了出来。我们的真实测试集包含15个不同人物（subjects）的8088帧真实深度图像，手工标注了稠密身体组件和7个上半身人体关节位置。我们也在[13]中的真实深度数据上进行了测试。结果表明在合成数据上出现的效果（effects）可以镜像到真实数据上；进一步的，因为在姿势和体型上的极端变化，到目前为止我们的合成测试数据集是最难的。因为在主要的娱乐场景中用户都是面向摄像机（0°）的，在绝大多数实验中我们将用户的旋转角度限制在±120°，尽管我们也评估了全 360°的情形。<br><strong>误差度量</strong><br>我们同时量化了分类和关节预测精度。我们给出了每类的平均精度，即真实组件标签与最可能推理组件标签之间模糊矩阵（confusion matrix）的对角线(diagonal，都不知道具体指什么)。虽然大小不同，该度量却同等地衡量每个身体组件，尽管对组件边界的误标注降低了绝对数量。</p>
<p>我们给出了以置信度阈值为自变量的关节估计的recall-precision（测全率-测准率，再现率-精度）曲线。我们量化了每个关节（预测）的平均精度，或者全部关节（预测）的平均的平均精度（mean Average Precision，mAP）。离真实位置D厘米内的第一个关节预测会当作真正（true positive）,而其他也在D厘米之内的预测则会当作假正（false positive）。这是惩罚正确位置附近的多个虚假预测，因为它们会使downstream跟踪算法变慢。所有D厘米之外的关节（预测）也视作假正。注意所有预测（不知是最可信的）都计算到度量中去。在图像中不可见的关节不会当作假负。下面我们设置D=0.1m，approximately the accuracy of the hand-labeled real test data ground truth（冒出这么一句，都不知道什么意思啊）。分类（classification）与关节预测准确性之间的强相关性（参见图6(a)和图8(a)中的蓝色曲线）表明下面从观察所得的针对某一个的趋势同样适用另一个。</p>
<h2 id="定性结果"><a href="#定性结果" class="headerlink" title="定性结果"></a>定性结果</h2><p><img src="/images/Kinect-Skeleton-5.png" alt=""><br>图5给出了我们算法的一些例子推理。请注意，在很大的身体和摄像机姿势、景深、cropping（裁剪）、身材和体型（如瘦小的小孩和壮硕的成年人）变化范围上都有很高的分类和关节预测准确性。底部一列给出了身体组件分类失败的一些形式（modes）。第一个例子是没能区分出深度图像中的细微变化，像交叉的手臂。常常（就像第二和第三个失败的例子）最可能的身体组件是不正确的，但是在分布P(c|I,x)中还是有足够多的正确概率团可以生成准确的（关节）预测。第四个是没能很好推广到未见过的姿势的例子。但是（设置好的）置信度防止了（gates）很差预测的出现，以再现率（recall）的损失为代价维持了高的精确性。</p>
<p>注意我们没有使用任何时间或运动约束。尽管如此，补充材料中视频序列每帧图像的结果表明几乎每个关节都被准确预测，且抖动惊人的小。</p>
<h2 id="分类准确度"><a href="#分类准确度" class="headerlink" title="分类准确度"></a>分类准确度</h2><p><img src="/images/Kinect-Skeleton-6.png" alt=""><br>我们研究了几个训练参数对分类准确性的影响。在合成测试集和真实测试集上的趋势高度相关，并且真实测试集一致地显得比合成测试集更“容易”，也是是因为其中的姿势变化少些。<br><img src="/images/Kinect-Skeleton-7.png" alt=""></p>
<h3 id="训练图像数量"><a href="#训练图像数量" class="headerlink" title="训练图像数量"></a>训练图像数量</h3><p>在图6(a)中，我们展示了准确性大约与随机生成图像数量之间呈对数关系，尽管训练图像增加到大约100k幅时准确性开始停止提高。下面就会展示，这种饱和很可能是3棵20层决策树构成的森林本身模型能力有限所致。</p>
<h3 id="轮廓图像"><a href="#轮廓图像" class="headerlink" title="轮廓图像"></a>轮廓图像</h3><p>图6(a)也给出了我们的方法在合成轮廓图像上表现出来的品质。在合成轮廓图像中，式1所计算出来的特征可以是带尺度（以平均深度度量）的、也可以是不带尺度的（固定深度）。采用2D衡量标准设定10像素真正（true positive）阈值，对应的预测准确性分别是：有尺度的为0.539mAP，不带尺度的为0.465mAP。尽管深度模糊明显使预测更艰难，这些结果仍表明了我们方法对其他图像形态（other imaging modalities）也有较好的适用性。</p>
<h3 id="树的深度"><a href="#树的深度" class="headerlink" title="树的深度"></a>树的深度</h3><p>使用15k和900k幅图像，图6(b)展示了树深度对测试准确性的影响。在所有训练参数中，深度似乎具有最显著的作用，因为它直接影响着分类器的建模能力。只使用15k幅训练图像时在17层左右就出现了过拟合，将训练集增多到900k则避免了过拟合。精度在20层处的较高梯度表明训练更多层能得到更好的结果，当然有附加的较小的计算代价加上较大的额外存储代价。从实用方面看，感兴趣的是，直到大约10层，训练集的大小关系不大，这告诉了我们一个高效训练策略。</p>
<h3 id="最大探针偏移"><a href="#最大探针偏移" class="headerlink" title="最大探针偏移"></a>最大探针偏移</h3><p>训练时允许的深度探针偏移范围对准确性有很大影响。图6(c)给出了在5k幅训练图像上的比较结果。这里，最大探针偏移(‘maximum probe offset’)指式1中u和v允许赋予的x和y坐标的最大绝对值。图中右侧的同心盒子是5个测试的最大偏移集，使用图像中左肩处的像素对它们进行了较准；最大的偏移几乎覆盖了整个身体。（记得最大偏移随像素的世界深度变化而缩放）。随着最大探针偏移增加，分类器可以使用更多空间上下文来做决定，尽管训练数据不够最终会导致过拟合，精度随最大探针偏移增大而提高，尽管在约129像素大小处就不再增大。</p>
<h2 id="联合预测精度"><a href="#联合预测精度" class="headerlink" title="联合预测精度"></a>联合预测精度</h2><p><img src="/images/Kinect-Skeleton-8.png" alt=""><br>图7给出了在合成测试集上的平均精度，达0.731mAP。我们对比了理想化的配置——即给出了真实身体组件标签的情况和使用推理出的身体组件的真实配置情况。我们确实付出了小小的代价来适用身体组件的中间表示，但是很多关节的推理结果有很高的准确性并且接近（理论上的）上界。在真实测试集上，我们有头、肩、肘和手的真实标签。在这些真实组件上mAP达0.984，而在推理出的身体组件上的mAP也达到了0.914。正如预期一样，在这个较易的测试集上，这些数据理所当然较高。</p>
<h3 id="与最近邻（方法）比较"><a href="#与最近邻（方法）比较" class="headerlink" title="与最近邻（方法）比较"></a>与最近邻（方法）比较</h3><p>为说明以组件方式识别姿势是需要的，并且使读者了解(calibrate)我们测试集的难度，在图8(a)中我们与“确切最近邻全身匹配方法”的两个变种进行了比较。首先，作为理想情况，将“真实测试骨架（ground truth test skeleton）”与使用优化刚性变换对齐到3D世界空间（optimal rigid translational alignment in 3D world space）的训练样本骨架进行比较。当然，在实际操作（即姿势估计）中是无法得到（真实）测试骨架的。作为可实现系统的例子，其次我们使用chamfer匹配[14]对比测试图像与训练样本。Chamfer匹配需计算深度边(depth edges)和12个方向量化值（bins）。为使chamfer匹配容易些。我们随时去掉已处理的(cropped)训练和测试图像。我们使用团（mass）的3D中心对齐图像，且发现进一步的局部刚性变换只会降低准确性。</p>
<p>我们采用组件识别的算法，直到150k幅训练图像的规模，都比理想骨架匹配方法扩展性更好（generalizes better）。正如上文提到的，我们使用更深的树能获得更好的结果，但是（仅使用20层深度的树）我们已经能鲁棒地推理出身体关节位置和自然地处理（cropping）和变换。最近邻chamfer匹配方法比我们算法的速度要慢得多（2fps）。层次匹配[14]确实快些，但是它需要大规模样本集来达到相当的准确性。</p>
<h3 id="与-13-比较"><a href="#与-13-比较" class="headerlink" title="与[13]比较"></a>与[13]比较</h3><p>[13]的作者提供的数据和结果可直接比较。他们的算法使用[28]给出的身体组件预测，然后利用运动和时间信息跟踪骨架。他们的数据来自于飞行时间（time-of-flight）深度像机，该像机具有跟我们的结构光线传感器（structured light sensor）完全不同的噪声统计特性。没对我们的训练数据和算法作任何改变，图8(b)显示出了在关节预测准确性上有可观的提高，且我们的算法快10倍以上。</p>
<h3 id="全旋转和多人情况"><a href="#全旋转和多人情况" class="headerlink" title="全旋转和多人情况"></a>全旋转和多人情况</h3><p>为评估360o旋转场景，我们在900k幅包含全旋转的图像上训练了一个森林，并在5k幅合成全旋转图像（with held out poses）上进行了测试。尽管有极大的左-右模糊，我们的系统还是获得了0.655mAP，这表明我们的分类器能准确学习到区分正面背面姿势的细微视觉线索。分类后的残余左-右不确定性可以通过多重假设自然传播给跟踪算法。因为逐像素分类器适用性强，就算没有在训练时包含多人的情况，我们的方法仍然能在包含多人的图像中预测出关节的位置。结果见图1和补充材料。</p>
<h3 id="快速预测"><a href="#快速预测" class="headerlink" title="快速预测"></a>快速预测</h3><p>基于简单的自底向上聚类，我们实现了一个更快生成预测的替代方法。与身体组件分类器联合，该方法在Xbox GPU上的速度为~200fps，在8核的当前桌面CPU上使用mean shift时的速度为~50fps。节省了计算量的情况下，尽管使用meanshift方法为0.731mAP，该方法在合成测试集上也取得了可比的0.677mAP（Given the computational savings, the 0.677 mAP achieved on the synthetic test set compares favorably to the 0.731 mAP of the mean shift approach..）。</p>
<h1 id="探讨"><a href="#探讨" class="headerlink" title="探讨"></a>探讨</h1><p>我们已经知道，从单深度图像超实时给出身体关节的3D位置有多大的准确性。我们引入身体组件识别作为人姿势估计的中间表示。使用了一个高度多样性的合成训练集从而能训练出很深的决策森林而不会过拟合，并学习出了姿势和。通过在（概率）密度（函数）中检测模式（mode）给出最终的带可信权重的3D关节预测集。实验结果显示了真实数据和合成数据之间、中间分类和最后的关节预测精度之间都存在很高的相关性。我们突出表明了将整体骨架分解成不同部件的重要性，并在具有竞争力的测试集上得到了最好的精度。</p>
<p>对于未来的工作，我们计划进一步研究源mocap数据的不变性，合成管道中潜在生成模式（the generative Model）的特性，以及特定组件的定义。是否存在一个具有相当效率的直接回归出关节位置的方法尚不得而知。也许隐变量的一个全局估计比如人的粗略方向能用来约束身体组件推理和消除局部姿势估计的模糊性。</p>
<p><strong>《Real-Time Human Pose Recognition in Parts from Single Depth Images》</strong>-Jamie Shotton  Andrew Fitzgibbon<br>Microsoft Research Cambridge &amp; Xbox Incubation</p>
<p>中文翻译<br>来源：网络 编辑：<a href="https://github.com/mindcont" target="_blank" rel="external">mindcont</a> 校对：<a href="https://github.com/mindcont" target="_blank" rel="external">mindcont</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="http://blog.mindcont.com/2016/05/10/Real-Time-Human-Pose-Recognition-in-Parts-from-Single-Depth-Images/" data-id="cio1gmebf00080s9oakcqdizz" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Kinect/">Kinect</a></div><div class="post-nav"><a href="/2016/05/09/The-basic-principle-of-Kinect-depth-imaging/" class="next">Kinect深度成像的基本原理</a></div><div id="disqus_thread"><script>var disqus_shortname = 'micro-era';
var disqus_identifier = '2016/05/10/Real-Time-Human-Pose-Recognition-in-Parts-from-Single-Depth-Images/';
var disqus_title = 'Kinect骨骼提取原理';
var disqus_url = 'http://blog.mindcont.com/2016/05/10/Real-Time-Human-Pose-Recognition-in-Parts-from-Single-Depth-Images/';
(function() {
  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//micro-era.disqus.com/count.js" async></script></div></div></div></div><div class="pure-u-1-4"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="http://blog.mindcont.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kinect/">Kinect</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/Kinect/" style="font-size: 15px;">Kinect</a> <a href="/tags/Raspberry-Pi/" style="font-size: 15px;">Raspberry Pi</a> <a href="/tags/安卓/" style="font-size: 15px;">安卓</a> <a href="/tags/ngrok/" style="font-size: 15px;">ngrok</a> <a href="/tags/TensorFlow/" style="font-size: 15px;">TensorFlow</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/05/10/Real-Time-Human-Pose-Recognition-in-Parts-from-Single-Depth-Images/">Kinect骨骼提取原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/09/The-basic-principle-of-Kinect-depth-imaging/">Kinect深度成像的基本原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/08/Raspberry-Pi-kernel-compile-and-firmware-upgrade/">树莓派内核编译与固件升级</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/05/RaspberryPi/">有趣的树莓派项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/05/04/how-to-use-ngrok-penetrate-internal-network/">如何搭建ngrok穿透内网</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/24/how-to-use-tensorflow/">如何使用 TensorFlow</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/16/kinect-develop/">快速入门 Kinect for Windows v2 开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/15/Kinect-Disassemble/">Xbox One Kinect完全拆解</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/01/android-open-project-3/">Android开源项目——优秀项目</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/04/01/android-open-project-5/">Android开源项目——优秀个人和团体</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//micro-era.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="https://www.mindcont.com/" title="脑控科技" target="_blank">脑控科技</a><ul></ul><a href="https://www.mindcont.com/training/index.html" title="安卓培训" target="_blank">安卓培训</a><ul></ul><a href="http://ai.mindcont.com/" title="人工智能" target="_blank">人工智能</a><ul></ul><a href="ftp://www.mindcont.com/" title="FTP文件共享" target="_blank">FTP文件共享</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">微纪元.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create','UA-74386502-3','auto');ga('send','pageview');
</script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>